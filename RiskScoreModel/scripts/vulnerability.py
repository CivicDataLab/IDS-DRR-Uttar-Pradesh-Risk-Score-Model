import numpy as np
import DEA
import pandas as pd
import os
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import jenkspy

path = os.getcwd() #+r"/HP/flood-data-ecosystem-Himachal-Pradesh"
master_variables = pd.read_csv(path+'/RiskScoreModel/data/MASTER_VARIABLES.csv')
master_variables_copy = master_variables.copy()


master_variables['sd_piped_hhds_pct'] = master_variables['sd_piped_hhds_pct']/master_variables['total_hhd']
master_variables['sd_nosanitation_hhds_pct'] = master_variables['sd_nosanitation_hhds_pct']/master_variables['total_hhd']
#master_variables['sum_aged_population'] = master_variables['sum_aged_population']/master_variables['sum_population']


#INPUT VARS
vulnerability_vars = [#"mean_sexratio",
                      "avg_electricity",
                      "sd_nosanitation_hhds_pct",
                      "sd_piped_hhds_pct",
                      #"sum_aged_population"
                      ]

#OUTPUT VARS
damage_vars = []  
'''
damage_vars = ["total_livestock_loss","schools_damaged",
                "person_dead","person_major_injury","structure_lost",
                 "health_centres_lost","Roadlength"]
'''
# Apply custom weights based on damage conditions
vulnerability_df = (master_variables[vulnerability_vars + ['timeperiod', 'object_id',]]).copy()

scaler = MinMaxScaler()
    # Fit scaler to the data and transform it
#vulnerability_df['landd_score'] = scaler.fit_transform(vulnerability_df[damage_vars]).sum(axis=1) + 1

neg_inputs = ["sd_nosanitation_hhds_pct"#,"sum_aged_population",
                      ]
'''
def apply_custom_weights(row, threshold=0.0001):
    # Custom weights multiplier for significant damage
    #custom_weight = row['landd_score']  # Adjust this value as needed
    custom_weight = np.power(row['landd_score'], 2)
    # Check if any damage variable exceeds the threshold
    if any(row[damage_vars] > threshold):
        # Apply custom weights to the outputs (damage vars)
        row[damage_vars + neg_inputs] *= custom_weight

    return row
'''

# Function to assign bins based on breaks
def assign_bin(value, breaks,date,object_id):
    for i in range(len(breaks)):
        if value <= breaks[i]:
            return i + 1  # Since bins start from 1
    return len(breaks)  # If value is greater than the last break

def assign_bin_with_handling(data, n_classes=5):
    try:
        # Perform Natural Jenks classification
        breaks = jenkspy.jenks_breaks(data, n_classes=n_classes)
        
        # Check for duplicate edges
        unique_breaks = np.unique(breaks)
        if len(unique_breaks) < len(breaks):
            # Reduce the number of bins and drop duplicates
            n_classes -= 1
            print(f"Warning : Duplicate bin edges detected. Reducing number of bins to {n_classes}.")
            return assign_bin_with_handling(data, n_classes=n_classes)  # Recursive call with fewer bins
        
        # Return the bins
        return pd.cut(data, bins=unique_breaks, labels=list(range(n_classes, 0, -1)), include_lowest=True)
    
    except ValueError as e:
        print(f"Error during binning: {e}")
        raise


vulnerability_df_months = []
for month in tqdm(vulnerability_df.timeperiod.unique()):
    print(month)    
    #vulnerability_df = vulnerability_df.apply(apply_custom_weights, axis=1)
    vulnerability_df_month = vulnerability_df[vulnerability_df.timeperiod == month]
    vulnerability_df_month = vulnerability_df_month.set_index('object_id')
    
    # Initialize MinMaxScaler
    #scaler = MinMaxScaler()
    # Fit scaler to the data and transform it
    vulnerability_df_month[vulnerability_vars] = scaler.fit_transform(vulnerability_df_month[vulnerability_vars])# + damage_vars])
  

    # Reverse certain input variables (as more input means more vulnerability)
    vulnerability_df_month['avg_electricity'] = 1 - vulnerability_df_month['avg_electricity']
    vulnerability_df_month['sd_piped_hhds_pct'] = 1 - vulnerability_df_month['sd_piped_hhds_pct']

    if not damage_vars:
        vulnerability_df_month['dummy_output'] = 1
        current_damage_vars = ['dummy_output']
    else:
        current_damage_vars = damage_vars

    # Reverse all output (damage) variables (as more output means less damage)
    vulnerability_df_month[damage_vars] = 1 - vulnerability_df_month[damage_vars]
    #vulnerability_df_month[vulnerability_vars] = np.round(vulnerability_df_month[vulnerability_vars + damage_vars], 8)

    # Input dict
    X = vulnerability_df_month[vulnerability_vars].T.to_dict('list')

    # Output dict
    y = vulnerability_df_month[current_damage_vars].T.to_dict('list')

    DMU = list(vulnerability_df_month.index)#.astype(int))

    df = DEA.CRS(DMU, X, y, orientation="input", dual=False)

    # Merge efficiency results
    vulnerability_df_month = vulnerability_df_month.reset_index().merge(df,
                                                                        left_on='object_id',
                                                                        right_on='DMU')
    vulnerability_df_month['efficiency'] = vulnerability_df_month['efficiency'].astype(float)

    vulnerability_df_month['efficiency'] = np.round(vulnerability_df_month['efficiency'], 6)
    # Perform Natural Jenks classification with 5 classes
    # Handle binning with potential non-unique edges
    try:
        vulnerability_df_month['vulnerability'] = assign_bin_with_handling(
            vulnerability_df_month['efficiency'], n_classes=5
        )
    except ValueError:
        print(f"Skipping month {month} due to binning error.")
        continue

    #vulnerability_df_month.to_csv(os.getcwd()+'/RiskScoreModel/data/vulnerability_df_month.csv', index=False)
    #breaks = jenkspy.jenks_breaks(vulnerability_df_month['efficiency'], n_classes=5)

    '''
    # Add a new column with the assigned bins
    vulnerability_df_month['vulnerability'] = pd.cut(vulnerability_df_month['efficiency'],
                                                     bins=breaks,
                                                     labels=[5, 4, 3, 2, 1],  # Low efficiency = More Vulnerability
                                                     include_lowest=True)
    '''
    vulnerability_df_months.append(vulnerability_df_month)

vulnerability = pd.concat(vulnerability_df_months)
#ulnerability['landd'] = vulnerability[damage_vars].sum(axis=1)
vulnerability = vulnerability.drop(columns=['dummy_output'], errors='ignore')

master_variables = master_variables_copy.merge(vulnerability[['timeperiod', 'object_id', 'efficiency', 'vulnerability'#,'landd_score'
                                                              ]],
                       on = ['timeperiod', 'object_id'])

master_variables.to_csv(path+r'/RiskScoreModel/data/factor_scores_l1_vulnerability.csv', index=False)



