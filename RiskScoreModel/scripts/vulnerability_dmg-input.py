import numpy as np
import DEA
import pandas as pd
import os
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
import jenkspy

path = os.getcwd()
master_variables = pd.read_csv(path + '/RiskScoreModel/data/MASTER_VARIABLES.csv')
master_variables_copy = master_variables.copy()

# -------------------------------------------------------------------
# 1) Normalize raw counts into “per‐capita” or “per‐unit” form, as before
# -------------------------------------------------------------------
master_variables['total_livestock_loss'] = (
    master_variables['total_livestock_loss'] / master_variables['sum_population']
)
master_variables['schools_damaged'] = (
    master_variables['schools_damaged'] / master_variables['schools_count']
)
master_variables['person_dead'] = (
    master_variables['person_dead'] / master_variables['sum_population']
)
master_variables['person_major_injury'] = (
    master_variables['person_major_injury'] / master_variables['sum_population']
)
master_variables['structure_lost'] = (
    master_variables['structure_lost'] / master_variables['sum_population']
)
master_variables['health_centres_lost'] = (
    master_variables['health_centres_lost'] / master_variables['sum_population']
)
master_variables['Roadlength'] = (
    master_variables['Roadlength'] / master_variables['road_length']
)

master_variables['block_piped_hhds_pct'] = (
    master_variables['block_piped_hhds_pct'] / master_variables['total_hhd']
)
master_variables['block_nosanitation_hhds_pct'] = (
    master_variables['block_nosanitation_hhds_pct'] / master_variables['total_hhd']
)
master_variables['sum_aged_population'] = (
    master_variables['sum_aged_population'] / master_variables['sum_population']
)

# -------------------------------------------------------------------
# 2) Define which variables are “input‐side” damage vs. vulnerability
# -------------------------------------------------------------------
vulnerability_vars = [
    "CVIAll_comp",           # “bad” vulnerability variables (higher ⇒ worse)
    "avg_electricity",       # “good” vulnerability variable (higher ⇒ better → needs inversion)
    "block_nosanitation_hhds_pct",  # “bad” (higher ⇒ worse)
    "block_piped_hhds_pct",   # “good” (higher ⇒ better → needs inversion)
    "sum_aged_population"    # “bad” (higher ⇒ worse)
]

damage_vars = [
    "total_livestock_loss",
    "schools_damaged",
    "person_dead",
    "person_major_injury",
    "structure_lost",
    "health_centres_lost",
    "Roadlength"
]

# Of the five vulnerability_vars above, the ones that are already “bad” (no inversion needed):
neg_inputs = [
    "CVIAll_comp",
    "block_nosanitation_hhds_pct",
    "sum_aged_population"
]
# The “good” vulnerability_vars that must be inverted before feeding to DEA:
good_to_invert = [
    "avg_electricity",
    "block_piped_hhds_pct"
]

# -------------------------------------------------------------------
# 3) Build a working DataFrame with exactly those columns + keys + `landd_score`
# -------------------------------------------------------------------
vuln_damage_df = master_variables[
    vulnerability_vars + damage_vars + ['timeperiod', 'object_id']
].copy()

# (A) Compute `landd_score` on the raw damage_vars (Sum of MinMax(damage_vars) + 1),
#     if you wish to continue weighting by landd_score.  This is optional; if you
#     do not want to weight damages by landd_score, you can skip this step.
scaler_pre = MinMaxScaler()
vuln_damage_df['landd_score'] = (
    scaler_pre.fit_transform(vuln_damage_df[damage_vars]).sum(axis=1) + 1
)

# (B) Optionally apply custom weights *before* final per‐month scaling:
def apply_custom_weights(row, threshold=0.0001):
    custom_weight = np.power(row['landd_score'], 2)
    # If any single raw damage exceeds the tiny threshold, multiply all damage_vars + neg_inputs
    if any(row[damage_vars] > threshold):
        row[damage_vars + neg_inputs] *= custom_weight
    return row

vuln_damage_df = vuln_damage_df.apply(apply_custom_weights, axis=1)

# -------------------------------------------------------------------
# 4) For each month, do the following:
#     a) Filter to that month and index by `object_id`
#     b) MinMax‐scale (damage_vars + all vulnerability_vars) together
#     c) Invert only the “good” vulnerability_vars so they become “bad inputs”
#     d) Build DEA inputs = damage_vars + neg_inputs + inverted “good” vars
#     e) Use a constant output of [1.0]
#     f) Run DEA (input‐oriented).  Lower efficiency ⇒ more vulnerability
#     g) Jenks‐classify the efficiency into 5 bins (lowest efficiency → vulnerability=5)
# -------------------------------------------------------------------
scaler_month = MinMaxScaler()
monthly_results = []

for month in tqdm(vuln_damage_df['timeperiod'].unique()):
    # (4a) Filter by month & set index
    dfm = vuln_damage_df[vuln_damage_df['timeperiod'] == month].copy()
    dfm = dfm.set_index('object_id')

    # (4b) Scale all damage_vars + vulnerability_vars at once (per‐month)
    cols_to_scale = damage_vars + vulnerability_vars
    dfm[cols_to_scale] = scaler_month.fit_transform(dfm[cols_to_scale])
    dfm[cols_to_scale] = dfm[cols_to_scale].round(8)

    # (4c) Invert only those “good” variables so that higher electricity or piped_hhds ⇒ *less* vulnerability
    dfm['avg_electricity'] = 1.0 - dfm['avg_electricity']
    dfm['block_piped_hhds_pct'] = 1.0 - dfm['block_piped_hhds_pct']

    # (Optional) If you want to re‐apply landd_score weighting AFTER scaling,
    # do it here—but be careful not to re‐scale afterward. E.g.:
    # for d in damage_vars:
    #     dfm[d] *= dfm['landd_score']**2

    # (4d) Build DEA input dictionary: use exactly the “bad inputs” as defined:
    #      damage_vars (higher ⇒ worse) +
    #      neg_inputs (already “bad”) +
    #      inverted good vars (now “bad”)
    dea_inputs = {}
    for obj in dfm.index:
        # Gather each DMU’s row for [damage_vars + neg_inputs + inverted goods]
        values = dfm.loc[obj, damage_vars + neg_inputs + good_to_invert].tolist()
        dea_inputs[obj] = values

    # (4e) Build a constant‐output dictionary so that DEA “minimizes inputs”
    dea_outputs = {obj: [1.0] for obj in dfm.index}

    DMUs = list(dfm.index)
    dea_result_df = DEA.CRS(
        DMUs,
        dea_inputs,
        dea_outputs,
        orientation="input",
        dual=False
    )

    # (4f) Merge efficiency back into dfm and round
    dfm = dfm.reset_index().merge(
        dea_result_df[['DMU', 'efficiency']],
        how='left',
        left_on='object_id',
        right_on='DMU'
    )
    dfm['efficiency'] = dfm['efficiency'].astype(float).round(6)

    # (4g) Jenks‐classify efficiency into 5 vulnerability bins:
    #     lowest efficiency → vulnerability=5 (worst), highest efficiency → vulnerability=1 (best)
    try:
        breaks = jenkspy.jenks_breaks(dfm['efficiency'], n_classes=5)
        dfm['vulnerability'] = pd.cut(
            dfm['efficiency'],
            bins=breaks,
            labels=[5, 4, 3, 2, 1],
            include_lowest=True
        )
    except ValueError:
        print(f"Skipping month {month} due to Jenks error.")
        continue

    monthly_results.append(dfm)

# Concatenate all months
all_vuln = pd.concat(monthly_results, ignore_index=True)

# Merge back into the original DataFrame
master_variables = master_variables_copy.merge(
    all_vuln[['timeperiod', 'object_id', 'efficiency', 'vulnerability', 'landd_score']],
    on=['timeperiod', 'object_id'],
    how='left'
)

# Write out to CSV
master_variables.to_csv(
    path + '/RiskScoreModel/data/factor_scores_l1_vulnerability_2.csv',
    index=False
)
